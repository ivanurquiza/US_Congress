{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Anomalous Trades in U.S. Congressional Stock Transactions\n",
    "\n",
    "**Master's Thesis - Economics, Universidad de San Andrés**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements unsupervised anomaly detection on congressional stock trades. The goal is to identify transactions that deviate significantly from typical trading patterns, which may warrant further investigation for potential informed trading.\n",
    "\n",
    "**Methodology:**\n",
    "- Isolation Forest (Liu et al., 2008): detects global outliers\n",
    "- Local Outlier Factor (Breunig et al., 2000): detects contextual outliers\n",
    "\n",
    "**Validation approach:**\n",
    "Since we lack ground-truth labels, we validate by testing whether flagged anomalies exhibit higher cumulative abnormal returns (CAR), consistent with informed trading.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "# Stats\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# ML / Preprocessing\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Anomaly Detection\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "\n",
    "# ============================================================\n",
    "# PLOT STYLE CONFIGURATION\n",
    "# Goal: paper-quality figures, clean aesthetic\n",
    "# ============================================================\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'grid.linestyle': '--',\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 13,\n",
    "    'axes.labelsize': 11,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 120,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight'\n",
    "})\n",
    "\n",
    "# Color palette - muted, professional\n",
    "COLORS = {\n",
    "    'primary': '#2C3E50',\n",
    "    'secondary': '#7F8C8D', \n",
    "    'accent': '#E74C3C',\n",
    "    'success': '#27AE60',\n",
    "    'warning': '#F39C12',\n",
    "    'dem': '#3498DB',\n",
    "    'rep': '#E74C3C',\n",
    "    'ind': '#95A5A6'\n",
    "}\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Loading and Cleaning\n",
    "\n",
    "The raw data comes from Stata and has several variables stored as strings that should be numeric. We handle this systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_raw = pd.read_parquet('congress_merged.parquet')\n",
    "print(f'Loaded {len(df_raw):,} observations, {df_raw.shape[1]} variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working copy\n",
    "df = df_raw.copy()\n",
    "\n",
    "# ============================================================\n",
    "# TYPE CONVERSION\n",
    "# Many numeric variables were exported as strings from Stata.\n",
    "# We convert them, coercing errors to NaN.\n",
    "# ============================================================\n",
    "\n",
    "numeric_vars = [\n",
    "    # Returns\n",
    "    'excess_return', 'return_t', 'abs_return_t', 'return_overnight', 'return_intraday',\n",
    "    # Momentum\n",
    "    'momentum_5d', 'momentum_20d', 'momentum_60d', 'momentum_252d',\n",
    "    # Volatility\n",
    "    'realized_vol_30d', 'parkinson_vol_30d', 'realized_vol_60d', \n",
    "    'vol_of_vol_60d', 'realized_vol_252d',\n",
    "    # Volume\n",
    "    'volume_t', 'dollar_volume_t', 'volume_ratio_30d', 'abnormal_volume_30d',\n",
    "    # Liquidity\n",
    "    'amihud_illiq_20d', 'roll_spread_30d', 'hl_spread_20d', 'zero_volume_days_30d',\n",
    "    # Risk\n",
    "    'beta_252d', 'r2_market_252d',\n",
    "    # Valuation\n",
    "    'market_cap', 'price', 'book_value', 'price_to_book', 'ev_to_ebitda',\n",
    "    # CAR\n",
    "    'car_raw_30d', 'car_capm_30d', 'car_raw_60d', 'car_capm_60d', \n",
    "    'car_raw_90d', 'car_capm_90d'\n",
    "]\n",
    "\n",
    "for var in numeric_vars:\n",
    "    if var in df.columns:\n",
    "        df[var] = pd.to_numeric(df[var], errors='coerce')\n",
    "\n",
    "# Date conversion\n",
    "if 'traded' in df.columns:\n",
    "    df['traded_date'] = pd.to_datetime(df['traded'], dayfirst=True, errors='coerce')\n",
    "    df['trade_year'] = df['traded_date'].dt.year\n",
    "    df['trade_month'] = df['traded_date'].dt.month\n",
    "\n",
    "if 'filed' in df.columns:\n",
    "    df['filed_date'] = pd.to_datetime(df['filed'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Filing delay: days between trade and disclosure\n",
    "# Longer delays may indicate strategic timing of disclosure\n",
    "if 'traded_date' in df.columns and 'filed_date' in df.columns:\n",
    "    df['filing_delay'] = (df['filed_date'] - df['traded_date']).dt.days\n",
    "    # Cap extreme values (some errors in data)\n",
    "    df.loc[df['filing_delay'] < 0, 'filing_delay'] = np.nan\n",
    "    df.loc[df['filing_delay'] > 365, 'filing_delay'] = np.nan\n",
    "\n",
    "print(f'Type conversion complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check on numeric conversion success\n",
    "print('Sample of converted numeric variables:')\n",
    "check_vars = ['car_raw_30d', 'beta_252d', 'realized_vol_30d', 'filing_delay']\n",
    "for v in check_vars:\n",
    "    if v in df.columns:\n",
    "        valid = df[v].notna().sum()\n",
    "        pct = valid / len(df) * 100\n",
    "        print(f'  {v}: {valid:,} valid ({pct:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dimensions\n",
    "print(f'Total trades: {len(df):,}')\n",
    "print(f'Unique politicians: {df[\"name\"].nunique():,}')\n",
    "print(f'Unique tickers: {df[\"ticker\"].nunique():,}')\n",
    "print(f'Date range: {df[\"trade_year\"].min():.0f} - {df[\"trade_year\"].max():.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIGURE 1: Trading Activity Over Time\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "yearly = df.groupby('trade_year').size()\n",
    "ax.bar(yearly.index, yearly.values, color=COLORS['primary'], edgecolor='white', linewidth=0.5)\n",
    "\n",
    "# Mark STOCK Act (2012)\n",
    "ax.axvline(2012, color=COLORS['accent'], linestyle='--', linewidth=1.5, alpha=0.8)\n",
    "ax.text(2012.2, ax.get_ylim()[1]*0.9, 'STOCK Act', fontsize=9, color=COLORS['accent'])\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Number of Trades')\n",
    "ax.set_title('Congressional Stock Trading Activity by Year')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig1_trading_activity.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIGURE 2: Distribution by Party and Chamber\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Party distribution\n",
    "# Note: party is likely coded as 1=Dem, 2=Rep in Stata\n",
    "party_map = {1: 'Democrat', 2: 'Republican', 3: 'Independent'}\n",
    "df['party_label'] = df['party'].map(party_map).fillna('Unknown')\n",
    "\n",
    "party_counts = df['party_label'].value_counts()\n",
    "party_colors = [COLORS['dem'] if 'Dem' in p else COLORS['rep'] if 'Rep' in p else COLORS['ind'] \n",
    "                for p in party_counts.index]\n",
    "\n",
    "axes[0].barh(party_counts.index, party_counts.values, color=party_colors, edgecolor='white')\n",
    "axes[0].set_xlabel('Number of Trades')\n",
    "axes[0].set_title('Trades by Party')\n",
    "\n",
    "# Chamber distribution\n",
    "chamber_map = {1: 'House', 2: 'Senate'}\n",
    "df['chamber_label'] = df['chamber'].map(chamber_map).fillna('Unknown')\n",
    "\n",
    "chamber_counts = df['chamber_label'].value_counts()\n",
    "axes[1].barh(chamber_counts.index, chamber_counts.values, color=COLORS['primary'], edgecolor='white')\n",
    "axes[1].set_xlabel('Number of Trades')\n",
    "axes[1].set_title('Trades by Chamber')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig2_party_chamber.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction type\n",
    "print('Transaction types:')\n",
    "print(df['transaction'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Variable Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for key variables\n",
    "key_vars = ['car_raw_30d', 'excess_return', 'abnormal_volume_30d', \n",
    "            'realized_vol_30d', 'momentum_20d', 'beta_252d', 'filing_delay']\n",
    "key_vars = [v for v in key_vars if v in df.columns]\n",
    "\n",
    "df[key_vars].describe().T.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIGURE 3: Distribution of Key Variables\n",
    "# Using kernel density + histogram for better visualization\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_vars = ['car_raw_30d', 'abnormal_volume_30d', 'realized_vol_30d', \n",
    "             'momentum_20d', 'beta_252d', 'filing_delay']\n",
    "plot_vars = [v for v in plot_vars if v in df.columns]\n",
    "\n",
    "for i, var in enumerate(plot_vars):\n",
    "    ax = axes[i]\n",
    "    data = df[var].dropna()\n",
    "    \n",
    "    # Winsorize at 1st/99th percentile for visualization\n",
    "    q01, q99 = data.quantile([0.01, 0.99])\n",
    "    data_plot = data[(data >= q01) & (data <= q99)]\n",
    "    \n",
    "    ax.hist(data_plot, bins=50, density=True, alpha=0.6, color=COLORS['primary'], edgecolor='white')\n",
    "    \n",
    "    # Add KDE\n",
    "    try:\n",
    "        data_plot.plot.kde(ax=ax, color=COLORS['accent'], linewidth=2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    ax.axvline(data.median(), color=COLORS['warning'], linestyle='--', linewidth=1.5, label='Median')\n",
    "    ax.set_title(var.replace('_', ' ').title())\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "# Remove empty subplots if any\n",
    "for j in range(len(plot_vars), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.suptitle('Distribution of Key Features (Winsorized 1%-99%)', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig3_distributions.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIGURE 4: Correlation Matrix\n",
    "# We expect high correlation within variable families \n",
    "# (e.g., different CAR windows, different volatility measures)\n",
    "# ============================================================\n",
    "\n",
    "# Select numeric columns with sufficient non-null values\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove identifiers and year variables\n",
    "exclude = ['trade_year', 'trade_month', 'congress', 'committee_id', 'committee_from_date']\n",
    "num_cols = [c for c in num_cols if c not in exclude and df[c].notna().sum() > len(df)*0.3]\n",
    "\n",
    "corr = df[num_cols].corr()\n",
    "\n",
    "# Create mask for upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 11))\n",
    "sns.heatmap(corr, mask=mask, cmap='RdBu_r', center=0, \n",
    "            square=True, linewidths=0.5, \n",
    "            cbar_kws={'shrink': 0.6, 'label': 'Correlation'},\n",
    "            vmin=-1, vmax=1, ax=ax)\n",
    "ax.set_title('Correlation Matrix of Numeric Variables', fontsize=13, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig4_correlation.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify highly correlated pairs (|r| > 0.8)\n",
    "# This informs feature selection: we should not include redundant variables\n",
    "\n",
    "high_corr = []\n",
    "for i in range(len(corr.columns)):\n",
    "    for j in range(i+1, len(corr.columns)):\n",
    "        r = corr.iloc[i, j]\n",
    "        if abs(r) > 0.8:\n",
    "            high_corr.append({\n",
    "                'var1': corr.columns[i],\n",
    "                'var2': corr.columns[j],\n",
    "                'correlation': round(r, 3)\n",
    "            })\n",
    "\n",
    "if high_corr:\n",
    "    print('Highly correlated pairs (|r| > 0.8):')\n",
    "    print(pd.DataFrame(high_corr).sort_values('correlation', ascending=False).to_string(index=False))\n",
    "else:\n",
    "    print('No pairs with |r| > 0.8 found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Engineering and Selection\n",
    "\n",
    "For the anomaly detection model, we select features that:\n",
    "1. Capture different aspects of trading behavior (returns, volume, timing)\n",
    "2. Avoid redundancy (one variable per highly-correlated family)\n",
    "3. Have theoretical relevance for detecting informed trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE SELECTION RATIONALE\n",
    "# ============================================================\n",
    "#\n",
    "# We select one representative variable per family to avoid\n",
    "# multicollinearity. The 30-day window is chosen as it balances\n",
    "# noise (shorter windows) vs dilution of signal (longer windows).\n",
    "#\n",
    "# Features:\n",
    "# - car_raw_30d: cumulative abnormal return, our key outcome\n",
    "# - abnormal_volume_30d: unusual trading activity in the stock\n",
    "# - realized_vol_30d: recent stock volatility\n",
    "# - momentum_20d: price trend before trade\n",
    "# - beta_252d: systematic risk exposure\n",
    "# - amihud_illiq_20d: liquidity proxy (informed traders may prefer liquid stocks)\n",
    "# - filing_delay: strategic disclosure timing\n",
    "# ============================================================\n",
    "\n",
    "features_model = [\n",
    "    'car_raw_30d',        # Abnormal return (outcome of interest)\n",
    "    'abnormal_volume_30d', # Volume anomaly at trade time\n",
    "    'realized_vol_30d',    # Recent volatility\n",
    "    'momentum_20d',        # Price momentum\n",
    "    'beta_252d',           # Market sensitivity\n",
    "    'amihud_illiq_20d',    # Illiquidity measure\n",
    "    'filing_delay'         # Days to disclose trade\n",
    "]\n",
    "\n",
    "# Check availability\n",
    "features_available = [f for f in features_model if f in df.columns]\n",
    "features_missing = [f for f in features_model if f not in df.columns]\n",
    "\n",
    "print(f'Features available: {len(features_available)}/{len(features_model)}')\n",
    "if features_missing:\n",
    "    print(f'Missing: {features_missing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare modeling dataset\n",
    "# Keep only rows with complete feature data\n",
    "\n",
    "df_model = df[['trade_id', 'name', 'party_label', 'chamber_label', 'ticker', \n",
    "               'transaction', 'traded_date', 'trade_year', 'committee'] + features_available].copy()\n",
    "\n",
    "print(f'Before dropping NAs: {len(df_model):,}')\n",
    "df_model = df_model.dropna(subset=features_available)\n",
    "print(f'After dropping NAs:  {len(df_model):,}')\n",
    "\n",
    "# Handle infinities\n",
    "X = df_model[features_available].copy()\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "valid_idx = X.dropna().index\n",
    "df_model = df_model.loc[valid_idx]\n",
    "X = X.loc[valid_idx]\n",
    "\n",
    "print(f'Final sample: {len(df_model):,} trades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SCALING\n",
    "# We use RobustScaler (median/IQR) instead of StandardScaler\n",
    "# because financial data often has heavy tails and outliers.\n",
    "# This prevents extreme values from dominating the scaling.\n",
    "# ============================================================\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f'Scaled feature matrix: {X_scaled.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Anomaly Detection Model\n",
    "\n",
    "### Methodological Choices\n",
    "\n",
    "**Why Isolation Forest + LOF?**\n",
    "\n",
    "- **Isolation Forest** (Liu et al., 2008): Identifies points that are easy to isolate in random partitions. Works well for detecting global outliers—trades that are unusual compared to the entire dataset.\n",
    "\n",
    "- **Local Outlier Factor** (Breunig et al., 2000): Compares local density around each point to its neighbors. Detects contextual anomalies—trades that are unusual relative to similar trades.\n",
    "\n",
    "Using both methods provides robustness: trades flagged by both are more likely to be genuinely anomalous.\n",
    "\n",
    "**Hyperparameters:**\n",
    "\n",
    "- `contamination = 0.05`: We assume ~5% of trades may be anomalous. This is conservative; we conduct sensitivity analysis below.\n",
    "- `n_estimators = 200`: Number of trees in IF. Literature suggests 100-300 is typically sufficient for convergence.\n",
    "- `n_neighbors = 50`: For LOF, we use ~1-2% of the dataset, balancing local sensitivity vs stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "CONTAMINATION = 0.05  # Expected proportion of anomalies\n",
    "RANDOM_STATE = 42     # For reproducibility\n",
    "\n",
    "# IF: n_estimators=200 is standard; diminishing returns beyond ~300\n",
    "N_ESTIMATORS = 200\n",
    "\n",
    "# LOF: n_neighbors as ~1% of sample, minimum 20\n",
    "N_NEIGHBORS = max(20, int(len(X_scaled) * 0.01))\n",
    "\n",
    "print(f'Configuration:')\n",
    "print(f'  Contamination: {CONTAMINATION}')\n",
    "print(f'  IF n_estimators: {N_ESTIMATORS}')\n",
    "print(f'  LOF n_neighbors: {N_NEIGHBORS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ISOLATION FOREST\n",
    "# ============================================================\n",
    "\n",
    "clf_if = IForest(\n",
    "    contamination=CONTAMINATION,\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    max_samples='auto',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf_if.fit(X_scaled)\n",
    "\n",
    "df_model['score_if'] = clf_if.decision_scores_\n",
    "df_model['anomaly_if'] = clf_if.labels_  # 1 = anomaly\n",
    "\n",
    "n_anom_if = df_model['anomaly_if'].sum()\n",
    "print(f'Isolation Forest: {n_anom_if:,} anomalies ({n_anom_if/len(df_model)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOCAL OUTLIER FACTOR\n",
    "# ============================================================\n",
    "\n",
    "clf_lof = LOF(\n",
    "    n_neighbors=N_NEIGHBORS,\n",
    "    contamination=CONTAMINATION,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf_lof.fit(X_scaled)\n",
    "\n",
    "df_model['score_lof'] = clf_lof.decision_scores_\n",
    "df_model['anomaly_lof'] = clf_lof.labels_\n",
    "\n",
    "n_anom_lof = df_model['anomaly_lof'].sum()\n",
    "print(f'LOF: {n_anom_lof:,} anomalies ({n_anom_lof/len(df_model)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMBINED SCORE\n",
    "# Normalize scores to [0,1] and average for a consensus measure.\n",
    "# Flag as anomaly only if BOTH methods agree (conservative).\n",
    "# ============================================================\n",
    "\n",
    "scaler_mm = MinMaxScaler()\n",
    "scores_norm = scaler_mm.fit_transform(df_model[['score_if', 'score_lof']])\n",
    "\n",
    "df_model['score_if_norm'] = scores_norm[:, 0]\n",
    "df_model['score_lof_norm'] = scores_norm[:, 1]\n",
    "df_model['score_combined'] = (scores_norm[:, 0] + scores_norm[:, 1]) / 2\n",
    "\n",
    "# Conservative flag: anomaly in BOTH methods\n",
    "df_model['anomaly_both'] = ((df_model['anomaly_if'] == 1) & \n",
    "                            (df_model['anomaly_lof'] == 1)).astype(int)\n",
    "\n",
    "n_anom_both = df_model['anomaly_both'].sum()\n",
    "print(f'Consensus (both methods): {n_anom_both:,} anomalies ({n_anom_both/len(df_model)*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement between methods\n",
    "agreement = (df_model['anomaly_if'] == df_model['anomaly_lof']).mean()\n",
    "print(f'IF-LOF agreement rate: {agreement*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Anomaly Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIGURE 5: Anomaly Score Distributions\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "\n",
    "# IF score\n",
    "axes[0].hist(df_model['score_if_norm'], bins=50, color=COLORS['primary'], \n",
    "             edgecolor='white', alpha=0.8)\n",
    "q95_if = df_model['score_if_norm'].quantile(0.95)\n",
    "axes[0].axvline(q95_if, color=COLORS['accent'], linestyle='--', linewidth=2, label='95th pctl')\n",
    "axes[0].set_xlabel('Anomaly Score (normalized)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Isolation Forest')\n",
    "axes[0].legend()\n",
    "\n",
    "# LOF score\n",
    "axes[1].hist(df_model['score_lof_norm'], bins=50, color=COLORS['primary'], \n",
    "             edgecolor='white', alpha=0.8)\n",
    "q95_lof = df_model['score_lof_norm'].quantile(0.95)\n",
    "axes[1].axvline(q95_lof, color=COLORS['accent'], linestyle='--', linewidth=2, label='95th pctl')\n",
    "axes[1].set_xlabel('Anomaly Score (normalized)')\n",
    "axes[1].set_title('Local Outlier Factor')\n",
    "axes[1].legend()\n",
    "\n",
    "# Combined\n",
    "axes[2].hist(df_model['score_combined'], bins=50, color=COLORS['primary'], \n",
    "             edgecolor='white', alpha=0.8)\n",
    "q95_comb = df_model['score_combined'].quantile(0.95)\n",
    "axes[2].axvline(q95_comb, color=COLORS['accent'], linestyle='--', linewidth=2, label='95th pctl')\n",
    "axes[2].set_xlabel('Anomaly Score (normalized)')\n",
    "axes[2].set_title('Combined Score')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle('Distribution of Anomaly Scores', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig5_score_distributions.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIGURE 6: IF vs LOF Score Comparison\n",
    "# Shows whether both methods identify similar trades as anomalous\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "# Plot normal points\n",
    "normal = df_model[df_model['anomaly_both'] == 0]\n",
    "ax.scatter(normal['score_if_norm'], normal['score_lof_norm'], \n",
    "           s=8, alpha=0.3, c=COLORS['secondary'], label='Normal')\n",
    "\n",
    "# Plot anomalies\n",
    "anomalies = df_model[df_model['anomaly_both'] == 1]\n",
    "ax.scatter(anomalies['score_if_norm'], anomalies['score_lof_norm'], \n",
    "           s=25, alpha=0.8, c=COLORS['accent'], label='Anomaly (both)', edgecolors='white', linewidth=0.5)\n",
    "\n",
    "# Reference lines at threshold\n",
    "thresh = 1 - CONTAMINATION\n",
    "ax.axvline(df_model['score_if_norm'].quantile(thresh), color=COLORS['primary'], \n",
    "           linestyle=':', alpha=0.5)\n",
    "ax.axhline(df_model['score_lof_norm'].quantile(thresh), color=COLORS['primary'], \n",
    "           linestyle=':', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Isolation Forest Score')\n",
    "ax.set_ylabel('LOF Score')\n",
    "ax.set_title('Model Agreement: IF vs LOF Scores')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Correlation annotation\n",
    "r = df_model['score_if_norm'].corr(df_model['score_lof_norm'])\n",
    "ax.text(0.05, 0.95, f'r = {r:.2f}', transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig6_if_vs_lof.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Validation: Do Anomalies Exhibit Higher Abnormal Returns?\n",
    "\n",
    "If our anomaly detection is capturing informed trading, we expect:\n",
    "- Anomalous trades should have **higher CAR** on average\n",
    "- The difference should be **statistically significant**\n",
    "\n",
    "This serves as indirect validation since we lack ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TABLE 1: CAR Comparison - Normal vs Anomalous Trades\n",
    "# ============================================================\n",
    "\n",
    "def compare_groups(df, group_col, outcome_col):\n",
    "    \"\"\"Compare outcome between groups with t-test.\"\"\"\n",
    "    g0 = df[df[group_col] == 0][outcome_col].dropna()\n",
    "    g1 = df[df[group_col] == 1][outcome_col].dropna()\n",
    "    \n",
    "    result = {\n",
    "        'Normal (n)': len(g0),\n",
    "        'Normal (mean)': g0.mean(),\n",
    "        'Normal (median)': g0.median(),\n",
    "        'Anomaly (n)': len(g1),\n",
    "        'Anomaly (mean)': g1.mean(),\n",
    "        'Anomaly (median)': g1.median(),\n",
    "        'Diff (mean)': g1.mean() - g0.mean(),\n",
    "        't-stat': stats.ttest_ind(g0, g1)[0],\n",
    "        'p-value': stats.ttest_ind(g0, g1)[1]\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Compare for different CAR windows\n",
    "car_vars = ['car_raw_30d', 'car_capm_30d', 'car_raw_60d', 'car_capm_60d']\n",
    "car_vars = [v for v in car_vars if v in df_model.columns]\n",
    "\n",
    "results = []\n",
    "for var in car_vars:\n",
    "    res = compare_groups(df_model, 'anomaly_both', var)\n",
    "    res['Outcome'] = var\n",
    "    results.append(res)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[['Outcome', 'Normal (n)', 'Anomaly (n)', 'Normal (mean)', \n",
    "                         'Anomaly (mean)', 'Diff (mean)', 't-stat', 'p-value']]\n",
    "\n",
    "print('\\nCAR Comparison: Normal vs Anomalous Trades')\n",
    "print('='*80)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIGURE 7: CAR Distribution by Anomaly Status\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4.5))\n",
    "\n",
    "# Left: Boxplot\n",
    "car_var = 'car_raw_30d' if 'car_raw_30d' in df_model.columns else car_vars[0]\n",
    "\n",
    "box_data = [df_model[df_model['anomaly_both']==0][car_var].dropna(),\n",
    "            df_model[df_model['anomaly_both']==1][car_var].dropna()]\n",
    "\n",
    "bp = axes[0].boxplot(box_data, labels=['Normal', 'Anomaly'], patch_artist=True,\n",
    "                     widths=0.6, showfliers=False)  # Hide outliers for clarity\n",
    "\n",
    "bp['boxes'][0].set_facecolor(COLORS['secondary'])\n",
    "bp['boxes'][1].set_facecolor(COLORS['accent'])\n",
    "for box in bp['boxes']:\n",
    "    box.set_alpha(0.7)\n",
    "\n",
    "axes[0].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[0].set_ylabel('CAR 30d')\n",
    "axes[0].set_title('Distribution of Abnormal Returns')\n",
    "\n",
    "# Right: KDE comparison\n",
    "normal_car = df_model[df_model['anomaly_both']==0][car_var].dropna()\n",
    "anomaly_car = df_model[df_model['anomaly_both']==1][car_var].dropna()\n",
    "\n",
    "# Clip for visualization\n",
    "clip_low, clip_high = -0.3, 0.3\n",
    "normal_clip = normal_car[(normal_car >= clip_low) & (normal_car <= clip_high)]\n",
    "anomaly_clip = anomaly_car[(anomaly_car >= clip_low) & (anomaly_car <= clip_high)]\n",
    "\n",
    "normal_clip.plot.kde(ax=axes[1], color=COLORS['secondary'], linewidth=2, label='Normal')\n",
    "anomaly_clip.plot.kde(ax=axes[1], color=COLORS['accent'], linewidth=2, label='Anomaly')\n",
    "\n",
    "axes[1].axvline(normal_car.mean(), color=COLORS['secondary'], linestyle='--', linewidth=1.5)\n",
    "axes[1].axvline(anomaly_car.mean(), color=COLORS['accent'], linestyle='--', linewidth=1.5)\n",
    "axes[1].axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "axes[1].set_xlabel('CAR 30d')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Density Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(clip_low, clip_high)\n",
    "\n",
    "plt.suptitle('Cumulative Abnormal Returns: Normal vs Anomalous Trades', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig7_car_comparison.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Sensitivity Analysis\n",
    "\n",
    "We test robustness of results to different `contamination` rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SENSITIVITY: Varying contamination rate\n",
    "# Key question: Do anomalies consistently show higher CAR?\n",
    "# ============================================================\n",
    "\n",
    "contamination_values = [0.01, 0.02, 0.05, 0.10, 0.15]\n",
    "sensitivity_results = []\n",
    "\n",
    "car_col = 'car_raw_30d' if 'car_raw_30d' in df_model.columns else features_available[0]\n",
    "\n",
    "for cont in contamination_values:\n",
    "    # IF\n",
    "    clf = IForest(contamination=cont, n_estimators=200, random_state=42)\n",
    "    clf.fit(X_scaled)\n",
    "    labels = clf.labels_\n",
    "    \n",
    "    n_anom = labels.sum()\n",
    "    car_normal = df_model.loc[labels == 0, car_col].mean()\n",
    "    car_anom = df_model.loc[labels == 1, car_col].mean()\n",
    "    \n",
    "    # T-test\n",
    "    g0 = df_model.loc[labels == 0, car_col].dropna()\n",
    "    g1 = df_model.loc[labels == 1, car_col].dropna()\n",
    "    _, pval = stats.ttest_ind(g0, g1) if len(g1) > 1 else (np.nan, np.nan)\n",
    "    \n",
    "    sensitivity_results.append({\n",
    "        'Contamination': cont,\n",
    "        'N Anomalies': n_anom,\n",
    "        '% Anomalies': n_anom / len(df_model) * 100,\n",
    "        'CAR Normal': car_normal,\n",
    "        'CAR Anomaly': car_anom,\n",
    "        'Diff': car_anom - car_normal,\n",
    "        'p-value': pval\n",
    "    })\n",
    "\n",
    "sens_df = pd.DataFrame(sensitivity_results)\n",
    "print('Sensitivity Analysis: Isolation Forest')\n",
    "print('='*70)\n",
    "print(sens_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIGURE 8: Sensitivity Analysis Visualization\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4.5))\n",
    "\n",
    "# Left: CAR difference by contamination\n",
    "axes[0].bar(range(len(sens_df)), sens_df['Diff'], color=COLORS['primary'], edgecolor='white')\n",
    "axes[0].set_xticks(range(len(sens_df)))\n",
    "axes[0].set_xticklabels([f\"{c:.0%}\" for c in sens_df['Contamination']])\n",
    "axes[0].axhline(0, color='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Contamination Rate')\n",
    "axes[0].set_ylabel('CAR Difference (Anomaly - Normal)')\n",
    "axes[0].set_title('Effect Size Across Contamination Rates')\n",
    "\n",
    "# Right: p-value by contamination\n",
    "axes[1].plot(range(len(sens_df)), sens_df['p-value'], 'o-', color=COLORS['primary'], \n",
    "             markersize=8, linewidth=2)\n",
    "axes[1].axhline(0.05, color=COLORS['accent'], linestyle='--', linewidth=1.5, label='α = 0.05')\n",
    "axes[1].axhline(0.01, color=COLORS['warning'], linestyle='--', linewidth=1.5, label='α = 0.01')\n",
    "axes[1].set_xticks(range(len(sens_df)))\n",
    "axes[1].set_xticklabels([f\"{c:.0%}\" for c in sens_df['Contamination']])\n",
    "axes[1].set_xlabel('Contamination Rate')\n",
    "axes[1].set_ylabel('p-value')\n",
    "axes[1].set_title('Statistical Significance')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, max(0.1, sens_df['p-value'].max() * 1.1))\n",
    "\n",
    "plt.suptitle('Sensitivity Analysis: Model Robustness', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig8_sensitivity.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Characterization of Anomalous Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TABLE 2: Feature Comparison - Normal vs Anomaly\n",
    "# Which features differentiate anomalous trades?\n",
    "# ============================================================\n",
    "\n",
    "feature_comparison = []\n",
    "\n",
    "for feat in features_available:\n",
    "    normal_mean = df_model[df_model['anomaly_both']==0][feat].mean()\n",
    "    anom_mean = df_model[df_model['anomaly_both']==1][feat].mean()\n",
    "    \n",
    "    # Percent difference\n",
    "    if normal_mean != 0:\n",
    "        pct_diff = (anom_mean - normal_mean) / abs(normal_mean) * 100\n",
    "    else:\n",
    "        pct_diff = np.nan\n",
    "    \n",
    "    # T-test\n",
    "    g0 = df_model[df_model['anomaly_both']==0][feat].dropna()\n",
    "    g1 = df_model[df_model['anomaly_both']==1][feat].dropna()\n",
    "    _, pval = stats.ttest_ind(g0, g1) if len(g1) > 1 else (np.nan, np.nan)\n",
    "    \n",
    "    feature_comparison.append({\n",
    "        'Feature': feat,\n",
    "        'Normal Mean': normal_mean,\n",
    "        'Anomaly Mean': anom_mean,\n",
    "        'Diff %': pct_diff,\n",
    "        'p-value': pval\n",
    "    })\n",
    "\n",
    "feat_df = pd.DataFrame(feature_comparison).sort_values('Diff %', key=abs, ascending=False)\n",
    "print('Feature Comparison: Normal vs Anomalous Trades')\n",
    "print('='*70)\n",
    "print(feat_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIGURE 9: Feature Importance\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "feat_df_sorted = feat_df.sort_values('Diff %')\n",
    "colors = [COLORS['accent'] if x > 0 else COLORS['dem'] for x in feat_df_sorted['Diff %']]\n",
    "\n",
    "ax.barh(feat_df_sorted['Feature'], feat_df_sorted['Diff %'], color=colors, edgecolor='white')\n",
    "ax.axvline(0, color='black', linewidth=0.8)\n",
    "ax.set_xlabel('Difference (%) Anomaly vs Normal')\n",
    "ax.set_title('What Characterizes Anomalous Trades?')\n",
    "\n",
    "# Mark significant features\n",
    "for i, (_, row) in enumerate(feat_df_sorted.iterrows()):\n",
    "    if row['p-value'] < 0.05:\n",
    "        ax.annotate('*', xy=(row['Diff %'], i), fontsize=14, ha='left' if row['Diff %'] > 0 else 'right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig9_feature_importance.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print('* indicates p < 0.05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Anomaly rate by party\n",
    "# ============================================================\n",
    "\n",
    "party_anomaly = df_model.groupby('party_label').agg({\n",
    "    'anomaly_both': ['sum', 'mean', 'count']\n",
    "}).round(4)\n",
    "party_anomaly.columns = ['N Anomalies', 'Anomaly Rate', 'Total Trades']\n",
    "\n",
    "print('Anomaly Rate by Party')\n",
    "print('='*50)\n",
    "print(party_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Top politicians by anomaly rate (min 20 trades)\n",
    "# ============================================================\n",
    "\n",
    "politician_stats = df_model.groupby('name').agg({\n",
    "    'anomaly_both': ['sum', 'mean'],\n",
    "    'score_combined': 'mean',\n",
    "    'trade_id': 'count',\n",
    "    'party_label': 'first'\n",
    "})\n",
    "politician_stats.columns = ['N_Anomalies', 'Anomaly_Rate', 'Avg_Score', 'Total_Trades', 'Party']\n",
    "politician_stats = politician_stats[politician_stats['Total_Trades'] >= 20]\n",
    "\n",
    "print('Top 15 Politicians by Anomaly Rate (min 20 trades)')\n",
    "print('='*70)\n",
    "print(politician_stats.nlargest(15, 'Anomaly_Rate').round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIGURE 10: Top 15 Politicians by Anomaly Rate\n",
    "# ============================================================\n",
    "\n",
    "top15 = politician_stats.nlargest(15, 'Anomaly_Rate').reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = [COLORS['dem'] if p == 'Democrat' else COLORS['rep'] if p == 'Republican' else COLORS['ind'] \n",
    "          for p in top15['Party']]\n",
    "\n",
    "bars = ax.barh(range(len(top15)), top15['Anomaly_Rate'] * 100, color=colors, edgecolor='white')\n",
    "ax.set_yticks(range(len(top15)))\n",
    "ax.set_yticklabels(top15['name'])\n",
    "ax.set_xlabel('Anomaly Rate (%)')\n",
    "ax.set_title('Politicians with Highest Anomaly Rates (min. 20 trades)')\n",
    "\n",
    "# Add trade count annotation\n",
    "for i, (_, row) in enumerate(top15.iterrows()):\n",
    "    ax.annotate(f\"n={row['Total_Trades']:.0f}\", \n",
    "                xy=(row['Anomaly_Rate']*100 + 0.5, i),\n",
    "                va='center', fontsize=9, color=COLORS['secondary'])\n",
    "\n",
    "# Legend\n",
    "legend_elements = [Line2D([0], [0], color=COLORS['dem'], lw=8, label='Democrat'),\n",
    "                   Line2D([0], [0], color=COLORS['rep'], lw=8, label='Republican')]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig10_top_politicians.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset with anomaly scores\n",
    "output_cols = ['trade_id', 'name', 'party_label', 'chamber_label', 'ticker', \n",
    "               'transaction', 'traded_date', 'trade_year', 'committee',\n",
    "               'score_if', 'score_lof', 'score_combined',\n",
    "               'anomaly_if', 'anomaly_lof', 'anomaly_both'] + features_available\n",
    "\n",
    "output_cols = [c for c in output_cols if c in df_model.columns]\n",
    "\n",
    "df_model[output_cols].to_csv('congress_anomaly_results.csv', index=False)\n",
    "print('Saved: congress_anomaly_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print('\\n' + '='*60)\n",
    "print('SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Sample size: {len(df_model):,} trades')\n",
    "print(f'Features: {len(features_available)}')\n",
    "print(f'Anomalies (IF): {df_model[\"anomaly_if\"].sum():,} ({df_model[\"anomaly_if\"].mean()*100:.1f}%)')\n",
    "print(f'Anomalies (LOF): {df_model[\"anomaly_lof\"].sum():,} ({df_model[\"anomaly_lof\"].mean()*100:.1f}%)')\n",
    "print(f'Anomalies (consensus): {df_model[\"anomaly_both\"].sum():,} ({df_model[\"anomaly_both\"].mean()*100:.2f}%)')\n",
    "print(f'\\nValidation:')\n",
    "if car_col in df_model.columns:\n",
    "    car_norm = df_model[df_model['anomaly_both']==0][car_col].mean()\n",
    "    car_anom = df_model[df_model['anomaly_both']==1][car_col].mean()\n",
    "    print(f'  CAR (normal):  {car_norm:.4f}')\n",
    "    print(f'  CAR (anomaly): {car_anom:.4f}')\n",
    "    print(f'  Difference:    {car_anom - car_norm:.4f}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes for Next Steps\n",
    "\n",
    "1. **Benchmark comparison (Ziobrowski)**: Build calendar-time portfolios and compute Fama-French alpha to compare aggregate abnormal returns.\n",
    "\n",
    "2. **Supervised characterization**: Use Random Forest with `anomaly_both` as target to identify which features best predict anomalous trades.\n",
    "\n",
    "3. **Committee analysis**: Test whether anomaly rates differ by committee membership, particularly for committees with access to market-moving information.\n",
    "\n",
    "4. **Time-series patterns**: Check if anomaly rates increased around specific events (financial crises, policy announcements)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
