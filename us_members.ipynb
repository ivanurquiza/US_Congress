{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08558480-881b-481c-9514-17d9a37b82e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚Üí Descargando legislators-historical.yaml...\n",
      "   ‚úÖ Descargado: 12225 miembros desde https://raw.githubusercontent.com/unitedstates/con...\n",
      "\n",
      "‚Üí Descargando legislators-current.yaml...\n",
      "   ‚úÖ Descargado: 537 miembros desde https://raw.githubusercontent.com/unitedstates/con...\n",
      "\n",
      "‚úÖ Dataset principal guardado: members_2020_2026.csv\n",
      "   Filas: 794\n",
      "   Columnas: 48\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CONGRESSIONAL MEMBERS DATASET BUILDER - HISTORICAL + CURRENT\n",
    "Per√≠odo: 2020-2026\n",
    "Fuente:  GitHub\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================\n",
    "\n",
    "ANALYSIS_START = '2020-01-01'\n",
    "ANALYSIS_END = '2026-12-31'\n",
    "RELEVANT_CONGRESSES = [116, 117, 118, 119]\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FUNCI√ìN AUXILIAR PARA DESCARGAR YAML\n",
    "# ============================================================\n",
    "\n",
    "def download_yaml(file_type):\n",
    "    \"\"\"\n",
    "    file_type: 'current' o 'historical'\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚Üí Descargando legislators-{file_type}.yaml...\")\n",
    "    \n",
    "    # Probar m√∫ltiples URLs\n",
    "    possible_urls = [\n",
    "        f'https://raw.githubusercontent.com/unitedstates/congress-legislators/master/legislators-{file_type}.yaml',\n",
    "        f'https://raw.githubusercontent.com/unitedstates/congress-legislators/main/legislators-{file_type}.yaml',\n",
    "        f'https://theunitedstates.io/congress-legislators/legislators-{file_type}.yaml'\n",
    "    ]\n",
    "    \n",
    "    for url in possible_urls:\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            data = yaml.safe_load(response.text)\n",
    "            print(f\"   ‚úÖ Descargado: {len(data)} miembros desde {url[:50]}...\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Fall√≥ {url[:50]}... ({e})\")\n",
    "            continue\n",
    "    \n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PASO 1: DESCARGAR AMBOS YAMLs\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# Descargar historical\n",
    "historical_yaml = download_yaml('historical')\n",
    "\n",
    "# Descargar current\n",
    "current_yaml = download_yaml('current')\n",
    "\n",
    "# Verificar que tenemos al menos uno\n",
    "if not historical_yaml and not current_yaml:\n",
    "    print(\"\\n‚ùå ERROR CR√çTICO: No se pudieron descargar los YAMLs\")\n",
    "    print(\"   Descarga manualmente desde:\")\n",
    "    print(\"   https://github.com/unitedstates/congress-legislators\")\n",
    "    exit(1)\n",
    "\n",
    "# Combinar\n",
    "all_legislators = []\n",
    "\n",
    "if historical_yaml:\n",
    "    for leg in historical_yaml:\n",
    "        leg['source'] = 'historical'\n",
    "        all_legislators.append(leg)\n",
    "\n",
    "if current_yaml:\n",
    "    for leg in current_yaml:\n",
    "        leg['source'] = 'current'\n",
    "        all_legislators.append(leg)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PASO 2: EXTRAER Y FILTRAR POR PER√çODO 2020-2026\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "members_data = []\n",
    "committees_data = []\n",
    "\n",
    "for legislator in all_legislators:\n",
    "    try:\n",
    "        # IDs\n",
    "        ids = legislator.get('id', {})\n",
    "        bioguide_id = ids.get('bioguide')\n",
    "        \n",
    "        if not bioguide_id:\n",
    "            continue\n",
    "        \n",
    "        # Info personal\n",
    "        name = legislator.get('name', {})\n",
    "        bio = legislator.get('bio', {})\n",
    "        \n",
    "        # T√©rminos\n",
    "        terms = legislator.get('terms', [])\n",
    "        \n",
    "        if not terms:\n",
    "            continue\n",
    "        \n",
    "        # *** FILTRAR T√âRMINOS POR PER√çODO 2020-2026 ***\n",
    "        relevant_terms = []\n",
    "        for term in terms:\n",
    "            term_start = pd.to_datetime(term.get('start'), errors='coerce')\n",
    "            term_end = pd.to_datetime(term.get('end'), errors='coerce')\n",
    "            \n",
    "            # Solo t√©rminos que se solapan con 2020-2026\n",
    "            if pd.notna(term_start) and pd.notna(term_end):\n",
    "                if term_start <= pd.Timestamp(ANALYSIS_END) and term_end >= pd.Timestamp(ANALYSIS_START):\n",
    "                    relevant_terms.append(term)\n",
    "        \n",
    "        # *** SI NO TIENE T√âRMINOS EN 2020-2026, SKIP ***\n",
    "        if not relevant_terms:\n",
    "            continue\n",
    "        \n",
    "        # Tomar el t√©rmino m√°s reciente del per√≠odo\n",
    "        latest_term = relevant_terms[-1]\n",
    "        \n",
    "        # Determinar si a√∫n sirve\n",
    "        latest_end = pd.to_datetime(latest_term.get('end'))\n",
    "        is_currently_serving = latest_end >= pd.Timestamp('2025-01-01')\n",
    "        \n",
    "        # Extraer info del miembro\n",
    "        member_record = {\n",
    "            # IDs\n",
    "            'bioguide_id': bioguide_id,\n",
    "            'govtrack_id': ids.get('govtrack'),\n",
    "            'opensecrets_id': ids.get('opensecrets'),\n",
    "            'fec_ids': str(ids.get('fec', [])),\n",
    "            'wikipedia_id': ids.get('wikipedia'),\n",
    "            'wikidata_id': ids.get('wikidata'),\n",
    "            \n",
    "            # Nombre\n",
    "            'first_name': name.get('first'),\n",
    "            'last_name': name.get('last'),\n",
    "            'middle_name': name.get('middle'),\n",
    "            'full_name': f\"{name.get('first', '')} {name.get('last', '')}\".strip(),\n",
    "            'official_full': name.get('official_full'),\n",
    "            'nickname': name.get('nickname'),\n",
    "            \n",
    "            # Bio\n",
    "            'birthday': bio.get('birthday'),\n",
    "            'gender': bio.get('gender'),\n",
    "            'religion': bio.get('religion'),\n",
    "            \n",
    "            # Info pol√≠tica del t√©rmino m√°s reciente en el per√≠odo\n",
    "            'party': latest_term.get('party'),\n",
    "            'state': latest_term.get('state'),\n",
    "            'type': latest_term.get('type'),\n",
    "            'district': latest_term.get('district'),\n",
    "            'senate_class': latest_term.get('class'),\n",
    "            'state_rank': latest_term.get('state_rank'),\n",
    "            \n",
    "            # Fechas\n",
    "            'first_relevant_term_start': relevant_terms[0].get('start'),\n",
    "            'latest_term_start': latest_term.get('start'),\n",
    "            'latest_term_end': latest_term.get('end'),\n",
    "            'num_relevant_terms': len(relevant_terms),\n",
    "            \n",
    "            # Status\n",
    "            'is_current': is_currently_serving,\n",
    "            'source': legislator.get('source'),\n",
    "            \n",
    "            # Contacto (del t√©rmino m√°s reciente)\n",
    "            'phone': latest_term.get('phone'),\n",
    "            'fax': latest_term.get('fax'),\n",
    "            'address': latest_term.get('address'),\n",
    "            'office': latest_term.get('office'),\n",
    "            'website': latest_term.get('url'),\n",
    "            'contact_form': latest_term.get('contact_form'),\n",
    "            'rss_url': latest_term.get('rss_url'),\n",
    "            \n",
    "            # Social media\n",
    "            'twitter': ids.get('twitter'),\n",
    "            'facebook': ids.get('facebook'),\n",
    "            'youtube': ids.get('youtube'),\n",
    "            'instagram': ids.get('instagram'),\n",
    "        }\n",
    "        \n",
    "        members_data.append(member_record)\n",
    "        \n",
    "        # Extraer comit√©s de TODOS los t√©rminos relevantes\n",
    "        for term in relevant_terms:\n",
    "            term_start = pd.to_datetime(term.get('start'))\n",
    "            term_end = pd.to_datetime(term.get('end'))\n",
    "            \n",
    "            # Calcular congresos\n",
    "            start_year = term_start.year\n",
    "            end_year = term_end.year\n",
    "            start_congress = (start_year - 1789) // 2 + 1\n",
    "            end_congress = (end_year - 1789) // 2 + 1\n",
    "            \n",
    "            # Comit√©s\n",
    "            for committee in term.get('committees', []):\n",
    "                for congress in range(start_congress, end_congress + 1):\n",
    "                    if congress in RELEVANT_CONGRESSES:\n",
    "                        committees_data.append({\n",
    "                            'bioguide_id': bioguide_id,\n",
    "                            'congress': congress,\n",
    "                            'committee': committee,\n",
    "                            'term_start': term.get('start'),\n",
    "                            'term_end': term.get('end'),\n",
    "                            'party': term.get('party'),\n",
    "                            'state': term.get('state')\n",
    "                        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# Crear DataFrames\n",
    "members_df = pd.DataFrame(members_data)\n",
    "committees_df = pd.DataFrame(committees_data)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PASO 3: FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "# Calcular edad\n",
    "members_df['birthday'] = pd.to_datetime(members_df['birthday'], errors='coerce')\n",
    "members_df['age_2024'] = 2024 - members_df['birthday'].dt.year\n",
    "members_df['age_2020'] = 2020 - members_df['birthday'].dt.year\n",
    "\n",
    "# Parsear fechas de t√©rminos\n",
    "members_df['first_relevant_term_start'] = pd.to_datetime(members_df['first_relevant_term_start'])\n",
    "members_df['latest_term_start'] = pd.to_datetime(members_df['latest_term_start'])\n",
    "members_df['latest_term_end'] = pd.to_datetime(members_df['latest_term_end'])\n",
    "\n",
    "# Calcular seniority (a√±os desde primer t√©rmino relevante)\n",
    "members_df['seniority_years'] = (\n",
    "    (members_df['latest_term_end'] - members_df['first_relevant_term_start']).dt.days / 365\n",
    ").round(2)\n",
    "\n",
    "# Chamber\n",
    "members_df['chamber'] = members_df['type'].map({\n",
    "    'sen': 'Senate',\n",
    "    'rep': 'House'\n",
    "})\n",
    "\n",
    "# Party clean\n",
    "members_df['party_clean'] = members_df['party'].map({\n",
    "    'Republican': 'R',\n",
    "    'Democrat': 'D',\n",
    "    'Independent': 'I'\n",
    "})\n",
    "\n",
    "# Flags\n",
    "members_df['is_senate'] = members_df['type'] == 'sen'\n",
    "members_df['is_house'] = members_df['type'] == 'rep'\n",
    "\n",
    "# Retirement info\n",
    "members_df['retirement_year'] = members_df['latest_term_end'].dt.year\n",
    "members_df['retired_during_period'] = (\n",
    "    (~members_df['is_current']) & \n",
    "    (members_df['retirement_year'] >= 2020)\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PASO 4: AGREGAR COMIT√âS\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "if not committees_df.empty:\n",
    "    # Lista de comit√©s √∫nicos por miembro\n",
    "    committees_by_member = committees_df.groupby('bioguide_id')['committee'].apply(\n",
    "        lambda x: '; '.join(sorted(set(x)))\n",
    "    ).reset_index()\n",
    "    committees_by_member.columns = ['bioguide_id', 'committees_list']\n",
    "    \n",
    "    # Contar\n",
    "    committees_count = committees_df.groupby('bioguide_id')['committee'].nunique().reset_index()\n",
    "    committees_count.columns = ['bioguide_id', 'num_committees']\n",
    "    \n",
    "    # Merge\n",
    "    members_df = members_df.merge(committees_by_member, on='bioguide_id', how='left')\n",
    "    members_df = members_df.merge(committees_count, on='bioguide_id', how='left')\n",
    "    \n",
    "    print(f\"‚úÖ Comit√©s agregados a {members_df['committees_list'].notna().sum()} miembros\")\n",
    "else:\n",
    "    members_df['committees_list'] = None\n",
    "    members_df['num_committees'] = 0\n",
    "\n",
    "# ============================================================\n",
    "# PASO 5: ORGANIZAR Y EXPORTAR\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "# Ordenar columnas\n",
    "column_order = [\n",
    "    # IDs\n",
    "    'bioguide_id', 'govtrack_id', 'opensecrets_id', 'fec_ids', 'wikipedia_id',\n",
    "    \n",
    "    # Personal\n",
    "    'full_name', 'first_name', 'last_name', 'middle_name', 'nickname',\n",
    "    'official_full', 'birthday', 'age_2024', 'age_2020', 'gender', 'religion',\n",
    "    \n",
    "    # Political\n",
    "    'party', 'party_clean', 'state', 'chamber', 'type', 'district',\n",
    "    'senate_class', 'state_rank',\n",
    "    \n",
    "    # Terms & Status\n",
    "    'first_relevant_term_start', 'latest_term_start', 'latest_term_end',\n",
    "    'num_relevant_terms', 'seniority_years', \n",
    "    'is_current', 'retirement_year', 'retired_during_period',\n",
    "    \n",
    "    # Committees\n",
    "    'committees_list', 'num_committees',\n",
    "    \n",
    "    # Flags\n",
    "    'is_senate', 'is_house',\n",
    "    \n",
    "    # Source\n",
    "    'source',\n",
    "    \n",
    "    # Contact\n",
    "    'phone', 'fax', 'address', 'office', 'website', 'contact_form', 'rss_url',\n",
    "    \n",
    "    # Social Media\n",
    "    'twitter', 'facebook', 'youtube', 'instagram'\n",
    "]\n",
    "\n",
    "existing_cols = [col for col in column_order if col in members_df.columns]\n",
    "members_final = members_df[existing_cols].copy()\n",
    "\n",
    "# Export principal\n",
    "output_file = 'members_2020_2026.csv'\n",
    "members_final.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úÖ Dataset principal guardado: {output_file}\")\n",
    "print(f\"   Filas: {len(members_final)}\")\n",
    "print(f\"   Columnas: {len(members_final.columns)}\")\n",
    "\n",
    "# Export comit√©s detallados\n",
    "if not committees_df.empty:\n",
    "    committees_output = 'committees_by_congress_2020_2026.csv'\n",
    "    committees_df.to_csv(committees_output, index=False)\n",
    "    print(f\"\\n‚úÖ Comit√©s detallados guardados: {committees_output}\")\n",
    "    print(f\"   Filas: {len(committees_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb75380c-9d39-47d5-8038-d765e3c04132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 230 comit√©s descargados\n",
      "   Miembros en dataset: 794\n",
      "   Miembros en comit√©s: 530\n",
      "   Match exitoso: 530\n",
      "   En dataset pero sin comit√©s: 264\n",
      "\n",
      "üìÅ Archivo guardado: members_2020_2026_WITH_COMMITTEES.csv\n",
      "\n",
      "üéØ ¬°DATASET COMPLETO CON COMIT√âS!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INTEGRAR COMIT√âS \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yaml\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 1. Cargar dataset existente\n",
    "\n",
    "members = pd.read_csv('members_2020_2026.csv')\n",
    "\n",
    "\n",
    "# 2. Descargar committee-membership\n",
    "\n",
    "url_membership = 'https://raw.githubusercontent.com/unitedstates/congress-legislators/master/committee-membership-current.yaml'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url_membership, headers=HEADERS, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    membership_data = yaml.safe_load(response.text)\n",
    "    print(f\"‚úÖ {len(membership_data)} comit√©s descargados\")\n",
    "except Exception as e:\n",
    "    \n",
    "    exit()\n",
    "\n",
    "# 3. Descargar committees-current para mapear IDs a nombres\n",
    "\n",
    "url_committees = 'https://raw.githubusercontent.com/unitedstates/congress-legislators/master/committees-current.yaml'\n",
    "\n",
    "committee_names = {}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url_committees, headers=HEADERS, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    committees_data = yaml.safe_load(response.text)\n",
    "    \n",
    "    # Crear mapeo de ID ‚Üí nombre\n",
    "    for committee in committees_data:\n",
    "        if isinstance(committee, dict):\n",
    "            thomas_id = committee.get('thomas_id', '')\n",
    "            name = committee.get('name', thomas_id)\n",
    "            \n",
    "            if thomas_id:\n",
    "                committee_names[thomas_id] = name\n",
    "    \n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  No se pudo descargar nombres de comit√©s: {e}\")\n",
    "    print(\"   Usando IDs en lugar de nombres\")\n",
    "\n",
    "# 4. Extraer membres√≠as\n",
    "\n",
    "\n",
    "bioguide_to_committees = {}\n",
    "total_memberships = 0\n",
    "\n",
    "for committee_id, members_list in membership_data.items():\n",
    "    # Intentar obtener nombre del comit√©\n",
    "    committee_name = committee_names.get(committee_id, committee_id)\n",
    "    \n",
    "    if isinstance(members_list, list):\n",
    "        for member in members_list:\n",
    "            if isinstance(member, dict) and 'bioguide' in member:\n",
    "                bioguide = member['bioguide']\n",
    "                \n",
    "                if bioguide not in bioguide_to_committees:\n",
    "                    bioguide_to_committees[bioguide] = []\n",
    "                \n",
    "                bioguide_to_committees[bioguide].append(committee_name)\n",
    "                total_memberships += 1\n",
    "\n",
    "\n",
    "\n",
    "# 5. Crear DataFrame de comit√©s\n",
    "\n",
    "committees_df = pd.DataFrame([\n",
    "    {\n",
    "        'bioguide_id': bioguide,\n",
    "        'committees_list': '; '.join(sorted(set(committees))),\n",
    "        'num_committees': len(set(committees))\n",
    "    }\n",
    "    for bioguide, committees in bioguide_to_committees.items()\n",
    "])\n",
    "\n",
    "\n",
    "# 6. Merge con dataset principal\n",
    "\n",
    "# Eliminar columnas viejas si existen\n",
    "if 'committees_list' in members.columns:\n",
    "    members = members.drop('committees_list', axis=1)\n",
    "if 'num_committees' in members.columns:\n",
    "    members = members.drop('num_committees', axis=1)\n",
    "\n",
    "# Merge\n",
    "members = members.merge(\n",
    "    committees_df,\n",
    "    on='bioguide_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rellenar NaN con 0 para num_committees\n",
    "members['num_committees'] = members['num_committees'].fillna(0).astype(int)\n",
    "\n",
    "# 7. Guardar\n",
    "\n",
    "output_file = 'members_2020_2026_WITH_COMMITTEES.csv'\n",
    "members.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# 8. Verificaci√≥n\n",
    "bioguides_in_dataset = set(members['bioguide_id'].values)\n",
    "bioguides_in_committees = set(bioguide_to_committees.keys())\n",
    "\n",
    "matched = bioguides_in_dataset & bioguides_in_committees\n",
    "\n",
    "print(f\"   Miembros en dataset: {len(bioguides_in_dataset)}\")\n",
    "print(f\"   Miembros en comit√©s: {len(bioguides_in_committees)}\")\n",
    "print(f\"   Match exitoso: {len(matched)}\")\n",
    "print(f\"   En dataset pero sin comit√©s: {len(bioguides_in_dataset - bioguides_in_committees)}\")\n",
    "\n",
    "print(f\"\\nüìÅ Archivo guardado: {output_file}\")\n",
    "print(f\"\\nüéØ ¬°DATASET COMPLETO CON COMIT√âS!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe490f3-fa39-4c3e-af18-cbbe05014103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìö SCRAPEANDO COMIT√âS DE WIKIPEDIA\n",
      "======================================================================\n",
      "\n",
      "‚Üí Scrapeando comit√©s...\n",
      "\n",
      "   Senate Finance...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   Senate Intelligence...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   Senate Armed Services...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   Senate Appropriations...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   Senate Foreign Relations...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   Senate Judiciary...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   House Ways and Means...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   House Appropriations...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   House Financial Services...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   House Energy and Commerce...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   House Judiciary...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   House Foreign Affairs...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "   House Armed Services...       Error: 403 Client Error: Forbidden for url: https://en.wi\n",
      "‚ö†Ô∏è  0 miembros\n",
      "\n",
      "‚ùå No se scrapearon datos\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRAPER DE WIKIPEDIA - COMIT√âS DEL CONGRESO\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìö SCRAPEANDO COMIT√âS DE WIKIPEDIA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# LISTA DE COMIT√âS A SCRAPEAR\n",
    "# ============================================================\n",
    "\n",
    "COMMITTEES = {\n",
    "    # Senate - URLs de p√°ginas individuales\n",
    "    'United_States_Senate_Committee_on_Finance': 'Senate Finance',\n",
    "    'United_States_Senate_Select_Committee_on_Intelligence': 'Senate Intelligence',\n",
    "    'United_States_Senate_Committee_on_Armed_Services': 'Senate Armed Services',\n",
    "    'United_States_Senate_Committee_on_Appropriations': 'Senate Appropriations',\n",
    "    'United_States_Senate_Committee_on_Foreign_Relations': 'Senate Foreign Relations',\n",
    "    'United_States_Senate_Committee_on_the_Judiciary': 'Senate Judiciary',\n",
    "    \n",
    "    # House\n",
    "    'United_States_House_Committee_on_Ways_and_Means': 'House Ways and Means',\n",
    "    'United_States_House_Committee_on_Appropriations': 'House Appropriations',\n",
    "    'United_States_House_Committee_on_Financial_Services': 'House Financial Services',\n",
    "    'United_States_House_Committee_on_Energy_and_Commerce': 'House Energy and Commerce',\n",
    "    'United_States_House_Committee_on_the_Judiciary': 'House Judiciary',\n",
    "    'United_States_House_Committee_on_Foreign_Affairs': 'House Foreign Affairs',\n",
    "    'United_States_House_Committee_on_Armed_Services': 'House Armed Services',\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# FUNCI√ìN DE SCRAPING\n",
    "# ============================================================\n",
    "\n",
    "def scrape_wikipedia_committee(wiki_page, committee_name):\n",
    "    \"\"\"\n",
    "    Scrapear miembros de una p√°gina de comit√© de Wikipedia\n",
    "    \"\"\"\n",
    "    url = f\"https://en.wikipedia.org/wiki/{wiki_page}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        members = []\n",
    "        \n",
    "        # Buscar secciones con \"Members\" o \"Membership\"\n",
    "        headers = soup.find_all(['h2', 'h3'], text=re.compile(r'Members|Membership', re.IGNORECASE))\n",
    "        \n",
    "        for header in headers:\n",
    "            # Buscar tabla despu√©s del header\n",
    "            table = header.find_next('table', class_='wikitable')\n",
    "            \n",
    "            if table:\n",
    "                rows = table.find_all('tr')[1:]  # Skip header row\n",
    "                \n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    \n",
    "                    if len(cells) >= 2:\n",
    "                        # Primera celda suele ser el nombre\n",
    "                        name_cell = cells[0]\n",
    "                        \n",
    "                        # Buscar enlace al miembro\n",
    "                        link = name_cell.find('a')\n",
    "                        \n",
    "                        if link:\n",
    "                            name = link.text.strip()\n",
    "                            member_url = link.get('href', '')\n",
    "                            \n",
    "                            # Extraer partido de la segunda celda o del texto\n",
    "                            party = None\n",
    "                            if len(cells) >= 2:\n",
    "                                party_text = cells[1].text.strip()\n",
    "                                if 'Democrat' in party_text or '(D' in party_text:\n",
    "                                    party = 'D'\n",
    "                                elif 'Republican' in party_text or '(R' in party_text:\n",
    "                                    party = 'R'\n",
    "                                elif 'Independent' in party_text or '(I' in party_text:\n",
    "                                    party = 'I'\n",
    "                            \n",
    "                            # Extraer estado si est√° disponible\n",
    "                            state = None\n",
    "                            if len(cells) >= 3:\n",
    "                                state_text = cells[2].text.strip()\n",
    "                                # Buscar c√≥digo de estado (2 letras)\n",
    "                                state_match = re.search(r'\\b([A-Z]{2})\\b', state_text)\n",
    "                                if state_match:\n",
    "                                    state = state_match.group(1)\n",
    "                            \n",
    "                            members.append({\n",
    "                                'name': name,\n",
    "                                'party': party,\n",
    "                                'state': state,\n",
    "                                'wiki_url': f\"https://en.wikipedia.org{member_url}\"\n",
    "                            })\n",
    "        \n",
    "        # Tambi√©n buscar listas de miembros (bullets)\n",
    "        if not members:\n",
    "            # Buscar listas <ul> despu√©s de headers de Members\n",
    "            for header in headers:\n",
    "                ul = header.find_next('ul')\n",
    "                \n",
    "                if ul:\n",
    "                    items = ul.find_all('li')\n",
    "                    \n",
    "                    for item in items:\n",
    "                        link = item.find('a')\n",
    "                        \n",
    "                        if link:\n",
    "                            name = link.text.strip()\n",
    "                            text = item.text\n",
    "                            \n",
    "                            # Extraer partido del texto\n",
    "                            party = None\n",
    "                            if '(D' in text or 'Democrat' in text:\n",
    "                                party = 'D'\n",
    "                            elif '(R' in text or 'Republican' in text:\n",
    "                                party = 'R'\n",
    "                            elif '(I' in text or 'Independent' in text:\n",
    "                                party = 'I'\n",
    "                            \n",
    "                            # Extraer estado\n",
    "                            state_match = re.search(r'\\(([A-Z]{2})\\)', text)\n",
    "                            state = state_match.group(1) if state_match else None\n",
    "                            \n",
    "                            members.append({\n",
    "                                'name': name,\n",
    "                                'party': party,\n",
    "                                'state': state,\n",
    "                                'wiki_url': f\"https://en.wikipedia.org{link.get('href', '')}\"\n",
    "                            })\n",
    "        \n",
    "        return members\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      Error: {str(e)[:50]}\")\n",
    "        return []\n",
    "\n",
    "# ============================================================\n",
    "# SCRAPING PRINCIPAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n‚Üí Scrapeando comit√©s...\\n\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for wiki_page, committee_name in COMMITTEES.items():\n",
    "    print(f\"   {committee_name}...\", end=' ')\n",
    "    \n",
    "    members = scrape_wikipedia_committee(wiki_page, committee_name)\n",
    "    \n",
    "    if members:\n",
    "        for member in members:\n",
    "            all_data.append({\n",
    "                'committee_name': committee_name,\n",
    "                'member_name': member['name'],\n",
    "                'party': member['party'],\n",
    "                'state': member['state'],\n",
    "                'wiki_url': member['wiki_url']\n",
    "            })\n",
    "        \n",
    "        print(f\"‚úÖ {len(members)} miembros\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  0 miembros\")\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting\n",
    "\n",
    "# ============================================================\n",
    "# GUARDAR RESULTADOS\n",
    "# ============================================================\n",
    "\n",
    "if all_data:\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    df.to_csv('wikipedia_committees_scraped.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ SCRAPING COMPLETADO\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nüìÅ Archivo: wikipedia_committees_scraped.csv\")\n",
    "    print(f\"   Total registros: {len(df)}\")\n",
    "    print(f\"   Comit√©s: {df['committee_name'].nunique()}\")\n",
    "    print(f\"   Miembros √∫nicos: {df['member_name'].nunique()}\")\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    print(f\"\\nüìä Por partido:\")\n",
    "    print(df['party'].value_counts())\n",
    "    \n",
    "    # Muestra\n",
    "    print(f\"\\nüìã Primeras 15 filas:\")\n",
    "    print(df.head(15).to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå No se scrapearon datos\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7893515-c41b-4a9c-b244-2314a00d1b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
